@Comment{jabref-meta: databaseType:bibtex;}

%------------------
% Websites

@misc{web_embedding-howto,
  author = {Martín Pellarolo},
  title = {How to use Pre-trained Word Embeddings in PyTorch},
  year = 2018,
  url = {https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76},
  urldate = {2021-05-16}
}

@misc{web_embedding-glove, 
  url = {https://nlp.stanford.edu/projects/glove/},
  urldate = {2021-05-16}
}

%[5] - MEDIUM
@misc{5_11-essential-NNs_2020,
  author = {Ye Andre},
  journal = "",
  publisher = "Medium",
  pages = 0,
  year = 2020,
  title = "11 Essential Neural Network Architectures, Visualized and Explained",
  doi = {},
  url = {https://medium.com/analytics-vidhya/11-essential-neural-network-architectures-visualized-explained-7fc7da3486d8}
}

%[6] - Medium
@misc{6_10-NN-architectures_2018,
  author = {Le James},
  journal = "",
  publisher = "Medium",
  pages = 0,
  year = 2018,
  title = "The 10 Neural Network Architectures Machine Learning Researchers Need To Learn",
  doi = {},
  url = {https://medium.com/cracking-the-data-science-interview/a-gentle-introduction-to-neural-networks-for-machine-learning-d5f3f8987786}
}

@misc{naru-github,
    author = {Daniel Nepp},
    title="Directed-NARU model implementation and experiments source code",
    url={https://github.com/Gleethos/NARU}
}

%[7] - Nautilus
@misc{7_dorment-brain_2015,
  author = {Kelly Clancy and Marina Muun},
  journal = "",
  publisher = "Nautilus Quaterly",  
  pages = 0,
  year = 2015,
  month = {august},
  day = 6,
  title = "Here’s Why Your Brain Seems Mostly Dormant",
  doi = {},
  url = {http://nautil.us/issue/27/dark-matter/heres-why-your-brain-seems-mostly-dormant}
}


%------------------
% Incollection

% incollection NOT WORKING! WHY?
@incollection{LeCunBOM12, 
  added-at = {2017-05-16T00:00:00.000+0200},
  author = {LeCun, Yann and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
  biburl = {https://www.bibsonomy.org/bibtex/2bdb95f0c55c51543f38f485288a0e4f7/dblp},
  booktitle = {Neural Networks: Tricks of the Trade (2nd ed.)}, 
  editor = {Montavon, Grégoire and Orr, Genevieve B. and Müller, Klaus-Robert},
  ee = {https://doi.org/10.1007/978-3-642-35289-8_3},
  interhash = {cc0d55e40d1d2e9b95b9308e79d6708c},
  intrahash = {bdb95f0c55c51543f38f485288a0e4f7},
  isbn = {978-3-642-35288-1},
  keywords = {dblp},
  pages = {9-48},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  timestamp = {2019-09-26T12:30:49.000+0200},
  title = {Efficient BackProp.},
  url = {http://dblp.uni-trier.de/db/series/lncs/lncs7700.html#LeCunBOM12},
  volume = 7700,
  year = 2012
}



%------------------
% Books

%[b-1] - Deep Learning
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    isbn = {978-0-262-03561-3},
    year={2016}
}


% [b-2] - Deep Learning - Gibson
@book{Patterson-Gibson-2017,
  abstract = {Although interest in machine learning has reached a high point, lofty expectations often scuttle projects before they get very far. How can machine learning---especially deep neural networks---make a real difference in your organization? This hands-on guide not only provides the most practical information available on the subject, but also helps you get started building efficient deep learning networks. The authors provide theory on deep learning before introducing their open-source Deeplearning4j (DL4J) library for developing production-class workflows. Through real-world examples, you will learn methods and strategies for training deep network architectures and running deep learning workflows on Spark and Hadoop with DL4J.},
  added-at = {2017-11-10T09:58:27.000+0100},
  address = {Beijing},
  author = {Patterson, Josh and Gibson, Adam},
  biburl = {https://www.bibsonomy.org/bibtex/2861e7eedb0f4c75409500abe08c9ad2d/flint63},
  file = {eBook:2017/PattersonGibson17.pdf:PDF;O'Reilly Product page:http\://shop.oreilly.com/product/0636920035343/:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/1491914254/:URL;Related Web site:https\://deeplearning4j.org/:URL},
  groups = {public},
  interhash = {aaa0845182fff81206ef3b2dbc0beaea},
  intrahash = {861e7eedb0f4c75409500abe08c9ad2d},
  isbn = {978-1-4919-1425-0},
  keywords = {01841 102 safari book numerical ai software development learn java tool},
  publisher = {O'Reilly},
  timestamp = {2018-04-16T11:39:52.000+0200},
  title = {Deep Learning: A Practitioner's Approach},
  url = {https://www.safaribooksonline.com/library/view/deep-learning/9781491924570/},
  username = {flint63},
  year = 2017
}

@book{nielsenneural,
  added-at = {2019-01-15T22:46:49.000+0100},
  author = {Nielsen, Michael A.},
  biburl = {https://www.bibsonomy.org/bibtex/274383acee84241145ff4ffede9658206/slicside},
  interhash = {04d527cadd39f888fc3babcad3343362},
  intrahash = {74383acee84241145ff4ffede9658206},
  keywords = {ba-2018-hahnrico},
  publisher = {Determination Press},
  timestamp = {2019-01-15T22:46:49.000+0100},
  title = {Neural Networks and Deep Learning},
  type = {misc},
  url = {http://neuralnetworksanddeeplearning.com/},
  isbn = {978-3-0300-6856-1},
  year = 2018
}



%------------------
% Articles & Papers

%[0] Hinton & Williams
@article{0_learning-representations-1986,
  added-at = {2019-05-21T10:10:49.000+0200},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  biburl = {https://www.bibsonomy.org/bibtex/2a392597c4f9cff2cd3c96c2191fa1eb6/sxkdz},
  doi = {10.1038/323533a0},
  interhash = {c354bc293fa9aa7caffc66d40a014903},
  intrahash = {a392597c4f9cff2cd3c96c2191fa1eb6},
  journal = {Nature},
  keywords = {imported},
  number = 6088,
  pages = {533--536},
  timestamp = {2019-05-21T10:10:49.000+0200},
  title = {{Learning Representations by Back-propagating Errors}},
  url = {http://www.nature.com/articles/323533a0},
  volume = 323,
  year = 1986
}

% [1]
@article{1_ANNs_2000,
  author = {I.A. Basheer and M. N. Hajmeer},
  journal = "Journal of Microbiological Methods",
  pages = 31,
  year = 2000,
  title = "Artificial neural networks: fundamentals, computing, design, and application.",
  doi = {10.1016/S0167-7012(00)00201-3},
  url = {https://pubmed.ncbi.nlm.nih.gov/11084225/}
}

%[2] 
@article{2_ANN-survey_2017,
  author = {Weibo Liua and Zidong Wang and Xiaohui Liu and Nianyin Zeng and Yurong Liu and Fuad E. Alsaadi},
  journal = "Elsevier",
  pages = 26,
  year = 2017,
  month = {july},
  day = {3},
  title = "A survey of deep neural network architectures and their applications.",
  doi = {},
  url = {}
}

%[3]
@article{3_ANN-introduction_1993,
  author = {Ben Kröse and Ben Krose and Patrick van der Smagt and Patrick Smagt},
  journal = "",
  pages = 0,
  year = 1993,
  title = "An Introduction To Neural Networks",
  doi = {10.1.1.18.493},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.493}
}

%[4]
@article{4_synthetic-gradients_2017,
  author = {Max Jaderberg and Wojciech Marian Czarnecki and Simon Osindero and Oriol Vinyals and Alex Graves and David Silver and Koray Kavukcuoglu},
  journal = "",
  pages = 0,
  year = 2017,
  title = "Decoupled Neural Interfaces using Synthetic Gradients",
  doi = {1608.05343},
  url = {https://arxiv.org/abs/1608.0534}
}

%[8]
@article{8_CDL-4-efficient_2015,
  author = {Priyadarshini Panda and Abhronil Sengupta and Kaushik Roy School of Electrical and Computer Engineering},
  journal = "",
  pages = 0,
  year = 2015,
  month = {september},
  title = "Conditional Deep Learning for Energy-Efficient and Enhanced Pattern Recognition",
  doi = {10.3850/9783981537079_0819},
  url = {https://academic.microsoft.com/paper/2267244539/reference/search?q=Conditional%20Deep%20Learning%20for%20energy-efficient%20and%20enhanced%20pattern%20recognition}
}

%[9] - Review
@article{9_deep-res-Learning_2015,
  author = {Kaiming He and Xiangyu and Shaoqing Ren and Jian Sun},
  journal = "",
  publisher = "Microsoft Researcher",
  pages = 0,
  year = 2015,
  month = {december},
  day = 10,
  title = "Deep Residual Learning for Image Recognition",
  doi = {1512.03385},
  url = {https://arxiv.org/abs/1512.03385}
}

%[10]
@article{10_rec-NN-Review_2015,
  author = {Zachary C. Lipton and John Berkowitz and Charles Elkan},
  journal = "",
  pages = 38,
  year = 2015,
  month = {june},
  day = 5,
  title = "A Critical Review of Recurrent Neural Networks for Sequence Learning",
  doi = {https://arxiv.org/abs/1506.00019},
  url = {https://arxiv.org/abs/1506.00019}
}

%[11]
@article{11_efficient-CDL_2017,
  author = {Priyadarshini Panda and Abhronil Sengupta and Kaushik Roy},
  journal = "",
  pages = 0,
  year = 2017,
  month = {february},
  title = "Energy-Efficient and Improved Image Recognition with Conditional Deep Learning",
  doi = {10.1145/3007192},
  url = {https://www.researchgate.net/publication/313594599_Energy-Efficient_and_Improved_Image_Recognition_with_Conditional_Deep_Learning}
}

%[12]
@article{12_dynamic-routing-in-ANNs_2017,
  author = {Mason McGill and Pietro Perona},
  journal = "",
  pages = 10,
  year = 2017,
  month = {september},
  title = "Dynamic Routing in Artificial Neural Networks",
  doi = {},
  url = {}
}

%[13]
@article{13_mastering-Go_2016,
  author = {David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George van denDriessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya},
  journal = "",
  pages = 17,
  year = 2016,
  month = {january},
  title = "Mastering the game of Go with deep neural networks and tree search",
  doi = {},
  url = {}
}


%[14] 
@article{14_sparsely-gated-experts_2017,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      journal="",
      month = {january}, 
      pages = {19},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


%[15] 
@article{15_dynamic-routing-between-capsules_2017,
      title={Dynamic Routing Between Capsules}, 
      author={Sara Sabour and Nicholas Frosst and Geoffrey E Hinton},
      year={2017},
      eprint={1710.09829},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


%[16]
@article{16_understanding-capsule-NNs_2017,
  author = {Pechyonkin Max},
  journal = "",
  publisher = "Medium",
  pages = 0,
  year = 2017,
  title = "Understanding Hinton’s Capsule Networks",
  doi = {},
  url = {}
}

%[17]
@article{17_survey-of-recent-CNNs_2019, 
   title={A survey of the recent architectures of deep convolutional neural networks},
   volume={53},
   ISSN={1573-7462},
   url={http://dx.doi.org/10.1007/s10462-020-09825-6},
   DOI={10.1007/s10462-020-09825-6},
   number={8},
   journal={Artificial Intelligence Review},
   publisher={Springer Science and Business Media LLC},
   author={Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
   year={2020},
   month={Apr},
   pages={5455–5516}
}


%[18]
@article{18_experimental-evidence-of-sparsity,
  author = {Alison L. Barth and James F.A. Poulet},
  journal = "Trends in Neurosciences",
  pages = 11,
  year = 2012,
  month = {june},
  title = "Experimental evidence for sparse firing in the neocortex",
  doi = {10.1016/j.tins.2012.03.008},
  url = {https://www.cell.com/trends/neurosciences/fulltext/S0166-2236(12)00051-3}
}

%[19]
@article{19_cost-of-cortical-computation,
  author = {Peter Lennie},
  journal = "Current Biology",
  pages = 7,
  year = 2012,
  month = {june},
  title = "The Cost of Cortical Computation",
  doi = {10.1016/s0960-9822(03)00135-0 },
  url = {https://www.cell.com/current-biology/fulltext/S0960-9822(03)00135-0}
}

%[20]
@article{20_catastrophic-inference,
  author = {Robert M. French},
  journal = "Psychology of Learning and Motivation",
  pages = 18,
  year = 1999,
  month = {may},
  title = "Catastrophic Forgetting in Connectionist Networks",
  doi = {10.1016/S0079-7421(08)60536-8},
  url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368}
}


%[21] - conference paper
@article{21_conditional-channel-gated-network,
  author = {Davide Abati and Jakub Tomczak and Tijmen Blankevoort and Simone Calderara and Rita Cucchiara and Babak Ehteshami Bejnordi},
  journal = "",
  pages = 10,
  year = 2020,
  month = {june},
  title = "Conditional Channel Gated Networks for Task Aware Continual Learning",
  doi = {10.1016/S0079-7421(08)60536-8},
  url = {https://www.researchgate.net/publication/343461550_Conditional_Channel_Gated_Networks_for_Task-Aware_Continual_Learning}
}


%[22]  
@article{22_era-of-big-data-processing,
      title={Era of Big Data Processing: A New Approach via Tensor Networks and Tensor Decompositions}, 
      author={Andrzej Cichocki},
      year={2014},
      month = {august},
      journal = "",
      pages = 30,
      eprint={1403.2048},
      archivePrefix={arXiv},
      primaryClass={cs.ET}, 
      url = {https://arxiv.org/abs/1403.2048}
}

%[23]
@article{23_low-rank-conditional-davis2014,
      title={Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks}, 
      author={Andrew Davis and Itamar Arel},
      year={2014},
      journal = "",
      eprint={1312.4461},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1312.4461v4}
}

%[24] - CDNN
@article{24_MoE-eigen2014,
      title={Learning Factored Representations in a Deep Mixture of Experts}, 
      author={David Eigen and Marc'Aurelio Ranzato and Ilya Sutskever},
      year={2014},
      journal = "",
      eprint={1312.4314},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%[25] - CDNN
@article{25_conditional-computation-bengio2016,
      title={Conditional Computation in Neural Networks for faster models}, 
      author={Emmanuel Bengio and Pierre-Luc Bacon and Joelle Pineau and Doina Precup},
      year={2016},
      journal = "",
      eprint={1511.06297},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%[26] - BATCHES - THIS IS BROKEN!!! TODO: FIX
@article{26_large-batch-keskar2017, 
  author    = {Nitish Shirish Keskar and
               Dheevatsa Mudigere and
               Jorge Nocedal and
               Mikhail Smelyanskiy and
               Ping Tak Peter Tang},
  title     = {On Large-Batch Training for Deep Learning: Generalization Gap and
               Sharp Minima},
  journal   = {CoRR},
  volume    = {abs/1609.04836},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.04836},
  archivePrefix = {arXiv},
  eprint    = {1609.04836},
  timestamp = {Mon, 13 Aug 2018 16:46:48 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KeskarMNST16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%[27] - CDLN
@article{27_path-net-evolution,
  author    = {Chrisantha Fernando and
               Dylan Banarse and
               Charles Blundell and
               Yori Zwols and
               David Ha and
               Andrei A. Rusu and
               Alexander Pritzel and
               Daan Wierstra},
  title     = {PathNet: Evolution Channels Gradient Descent in Super Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1701.08734},
  year      = {2017},
  url       = {http://arxiv.org/abs/1701.08734},
  archivePrefix = {arXiv},
  eprint    = {1701.08734},
  timestamp = {Mon, 13 Aug 2018 16:49:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FernandoBBZHRPW17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% -> EVIDENCE OF RECURRENCE IN BRAIN!
%[28]
@article{28_recurrence-in-BNN-Kietzmann_2019,
   title={Recurrence is required to capture the representational dynamics of the human visual system},
   volume={116},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.1905544116},
   DOI={10.1073/pnas.1905544116},
   number={43},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={Kietzmann, Tim C. and Spoerer, Courtney J. and Sörensen, Lynn K. A. and Cichy, Radoslaw M. and Hauk, Olaf and Kriegeskorte, Nikolaus},
   year={2019},
   month={Oct},
   pages={21854–21863}
}
%[29]
@article {29_recurrence-in-BNN-Kohitij354753,
	author = {Kohitij, Kar and Jonas, Kubilius and Kailyn, Schmidt and Issa, Elias B. and DiCarlo, James J.},
	title = {Evidence that recurrent circuits are critical to the ventral stream{\textquoteright}s execution of core object recognition behavior},
	elocation-id = {354753},
	year = {2018},
	doi = {10.1101/354753},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Non-recurrent deep convolutional neural networks (DCNNs) are currently the best models of core object recognition; a behavior supported by the densely recurrent primate ventral stream, culminating in the inferior temporal (IT) cortex. Are these recurrent circuits critical to ventral stream{\textquoteright}s execution of this behavior? We reasoned that, if recurrence is critical, then primates should outperform feedforward-only DCNNs for some images, and that these images should require additional processing time beyond the feedforward IT response. Here we first used behavioral methods to discover hundreds of these {\textquotedblleft}challenge{\textquotedblright} images. Second, using large-scale IT electrophysiology in animals performing core recognition tasks, we observed that behaviorally-sufficient, linearly-decodable object identity solutions emerged ~30ms (on average) later in IT for challenge images compared to DCNN and primate performance-matched {\textquotedblleft}control{\textquotedblright} images. We observed these same late solutions even during passive viewing. Third, consistent with a failure of feedforward computations, the behaviorally-critical late-phase IT population response patterns evoked by the challenge images were poorly predicted by DCNN activations. Interestingly, deeper CNNs better predicted these late IT responses, suggesting a functional equivalence between recurrence and additional nonlinear transformations. Our results argue that automatically-evoked recurrent circuits are critical even for rapid object identification. By precisely comparing current DCNNs, primate behavior and IT population dynamics, we provide guidance for future recurrent model development.},
	URL = {https://www.biorxiv.org/content/early/2018/06/26/354753},
	eprint = {https://www.biorxiv.org/content/early/2018/06/26/354753.full.pdf},
	journal = {bioRxiv}
}

%[30] - NNs are typically batch learner
@article{30_online-deep-learning,
  author    = {Doyen Sahoo and
               Quang Pham and
               Jing Lu and
               Steven C. H. Hoi},
  title     = {Online Deep Learning: Learning Deep Neural Networks on the Fly},
  journal   = {CoRR},
  volume    = {abs/1711.03705},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.03705},
  archivePrefix = {arXiv},
  eprint    = {1711.03705},
  timestamp = {Mon, 13 Aug 2018 16:48:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-03705.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% [31]

@article{31_continual-learning-2020,
      title={Continual Learning Using Multi-view Task Conditional Neural Networks}, 
      author={Honglin Li and Payam Barnaghi and Shirin Enshaeifar and Frieder Ganz},
      year={2020},
      eprint={2005.05080},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% [32] - MISH!!
@article{32_mish-misra2020,
      title={Mish: A Self Regularized Non-Monotonic Activation Function}, 
      author={Diganta Misra},
      year={2020},
      eprint={1908.08681},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

